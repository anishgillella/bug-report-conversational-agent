"""
LLM chat module for bug reporting via OpenRouter API.
Pure LLM-driven conversation with minimal hardcoding.
"""
import json
from datetime import datetime
from typing import Optional, List, Dict, Any
from openai import OpenAI

from config import Config
from models import BugReport, ConversationOutput, ExtractedBugInfo, ConversationEndSignal
from prompts import ConversationPrompts, ExtractionPrompts, ToolDefinitions
from data_manager import DataManager


class BugReportingBot:
    """LLM-driven bot for bug reporting. All conversation text generated by LLM."""
    
    def __init__(self, data_manager: DataManager):
        """Initialize the bot with OpenRouter client and data manager."""
        # Validate configuration
        Config.validate()
        
        self.data_manager = data_manager
        
        # Initialize OpenRouter client
        client_config = Config.get_api_client_config()
        self.client = OpenAI(**client_config)
        self.model = Config.OPENROUTER_MODEL
        
        # Conversation state
        self.messages: List[Dict[str, Any]] = []
        self.turn_count = 0
        self.max_turns = Config.MAX_CONVERSATION_TURNS
        
        # Gathered information (extracted from conversation)
        self.developer_id: Optional[int] = None
        self.selected_bug_id: Optional[int] = None
        self.progress_note: Optional[str] = None
        self.status: Optional[str] = None
        self.solved: Optional[bool] = None
        
        # Conversation trace
        self.trace: List[Dict[str, Any]] = []
        self.completed_reports: List[BugReport] = []
    
    def _get_system_prompt(self) -> str:
        """System prompt - guides LLM to conduct natural bug reporting conversation."""
        return ConversationPrompts.get_system_prompt()
    
    def _get_tools(self) -> List[Dict[str, Any]]:
        """Define available tools for the LLM."""
        return ToolDefinitions.get_tools()
    
    def _execute_tool(self, tool_name: str, tool_input: Dict[str, Any]) -> str:
        """Execute a tool and return result as JSON string."""
        if tool_name == "verify_developer":
            name = tool_input.get("name", "")
            developer = self.data_manager.find_developer_by_name(name)
            
            if developer:
                self.developer_id = developer["developer_id"]
                return json.dumps({
                    "success": True,
                    "developer_id": developer["developer_id"],
                    "name": developer["name"]
                })
            
            # Try to find partial matches
            similar = self.data_manager.find_similar_developers(name)
            if similar:
                if len(similar) == 1:
                    return json.dumps({
                        "type": "partial_match_needs_confirmation",
                        "confirmation_required": True,
                        "message": f'I found a partial match for your name. Did you mean "{similar[0]["name"]}"? Please confirm with yes or no.',
                        "suggested_name": similar[0]["name"],
                        "developer_id": similar[0]["developer_id"]
                    })
                else:
                    names = [d["name"] for d in similar]
                    return json.dumps({
                        "type": "multiple_matches",
                        "message": f"I found multiple developers with similar names: {', '.join(names)}. Please clarify which one is correct.",
                        "options": names
                    })
            
            # No matches
            all_developers = self.data_manager.developers
            names = [d["name"] for d in all_developers]
            return json.dumps({
                "success": False,
                "message": f"Developer '{name}' not found. Valid developers are: {', '.join(names)}"
            })
        
        elif tool_name == "get_bugs_for_developer":
            developer_id = tool_input.get("developer_id")
            bugs = self.data_manager.get_bugs_for_developer(developer_id)
            return json.dumps({
                "success": True,
                "bugs": bugs
            })
        
        return json.dumps({"error": "Unknown tool"})
    
    def get_bot_response(self) -> str:
        """Get response from LLM - handles tools and returns conversation text."""
        messages_with_system = [
            {"role": "system", "content": self._get_system_prompt()}
        ] + self.messages
        
        response = self.client.chat.completions.create(
            model=self.model,
            messages=messages_with_system,
            tools=self._get_tools(),
            max_tokens=Config.LLM_MAX_TOKENS
        )
        
        assistant_message = response.choices[0].message
        
        # Handle tool calls
        if hasattr(assistant_message, 'tool_calls') and assistant_message.tool_calls:
            # Add assistant's tool call to messages
            self.messages.append({
                "role": "assistant",
                "content": assistant_message.content or "",
                "tool_calls": [
                    {
                        "id": tc.id,
                        "type": "function",
                        "function": {
                            "name": tc.function.name,
                            "arguments": tc.function.arguments
                        }
                    }
                    for tc in assistant_message.tool_calls
                ]
            })
            
            # Execute tools and collect results
            for tool_call in assistant_message.tool_calls:
                tool_result = self._execute_tool(
                    tool_call.function.name,
                    json.loads(tool_call.function.arguments)
                )
                
                # Add tool result to messages
                self.messages.append({
                    "role": "tool",
                    "tool_call_id": tool_call.id,
                    "content": tool_result
                })
            
            # Get follow-up response after tool execution
            follow_up_response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": self._get_system_prompt()}
                ] + self.messages,
                tools=self._get_tools(),
                max_tokens=Config.LLM_MAX_TOKENS
            )
            
            follow_up_message = follow_up_response.choices[0].message
            self.messages.append({
                "role": "assistant",
                "content": follow_up_message.content
            })
            
            return follow_up_message.content or ""
        
        # No tool calls - just regular response
        self.messages.append({
            "role": "assistant",
            "content": assistant_message.content
        })
        
        return assistant_message.content or ""
    
    def _analyze_conversation_for_reports(self) -> List[BugReport]:
        """
        FINAL ANALYSIS: Analyze entire conversation and extract all completed bug reports.
        This is done ONCE at the end, after conversation is complete.
        """
        print("\n[DEBUG] Starting final analysis...")
        # Build full conversation text
        conv_text = ""
        for msg in self.messages:
            role = msg.get("role", "").upper()
            content = msg.get("content", "").strip()
            conv_text += f"{role}: {content}\n"
        
        # Get final analysis prompt
        analysis_prompt = ExtractionPrompts.get_final_analysis_prompt().format(
            conversation_text=conv_text
        )
        
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": analysis_prompt}],
                max_tokens=Config.EXTRACTION_MAX_TOKENS * 2  # More tokens for multiple reports
            )
            
            extracted_text = response.choices[0].message.content.strip()
            print(f"[DEBUG_EXTRACT] LLM returned: {extracted_text}")
            
            # Find and parse JSON array - handle markdown code blocks
            # Remove markdown code blocks if present
            if '```' in extracted_text:
                # Extract content between ``` markers
                parts = extracted_text.split('```')
                if len(parts) >= 2:
                    extracted_text = parts[1]  # Get content between first ``` and second ```
                    if extracted_text.startswith('json'):
                        extracted_text = extracted_text[4:]  # Remove 'json' language tag
                    extracted_text = extracted_text.strip()
            
            # Find and parse JSON array
            start = extracted_text.find('[')
            end = extracted_text.rfind(']') + 1
            
            if start >= 0 and end > start:
                json_str = extracted_text[start:end]
                parsed_list = json.loads(json_str)
                
                # Convert to BugReport objects
                reports = []
                for item in parsed_list:
                    try:
                        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                        report = BugReport(
                            bug_id=item['bug_id'],
                            progress_note=f"{timestamp} - {item['progress_note']}",
                            status=item['status'],
                            solved=item['solved']
                        )
                        reports.append(report)
                    except (KeyError, ValueError) as e:
                        pass
                
                return reports
        except Exception:
            pass
        
        return []
    
    def _should_end_conversation(self) -> bool:
        """Let LLM detect if conversation should end using Pydantic model."""
        if not self.messages:
            return False
        
        # Note: In the new two-stage architecture, completed_reports is empty during conversation
        # We detect end purely based on LLM's analysis of the conversation flow
        
        # Get recent conversation
        recent = self.messages[-3:]
        recent_text = "\n".join([f"{m.get('role')}: {m.get('content', '')[:100]}" for m in recent])
        
        # Get prompt from prompts module
        end_prompt = ExtractionPrompts.get_conversation_end_prompt().format(recent_text=recent_text)
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": end_prompt}],
                max_tokens=Config.END_DETECTION_MAX_TOKENS
            )
            extracted_text = response.choices[0].message.content.strip()
            
            # Find and parse JSON
            start = extracted_text.find('{')
            end = extracted_text.rfind('}') + 1
            
            if start >= 0 and end > start:
                json_str = extracted_text[start:end]
                parsed = json.loads(json_str)
                signal = ConversationEndSignal(**parsed)
                return signal.should_end
        except Exception:
            pass
        
        return False
    
    def _get_final_summary(self) -> str:
        """Generate a final summary of all completed reports using Pydantic models."""
        if not self.completed_reports:
            return "No bug reports were completed in this session."
        
        summary_lines = ["Here's the summary of your bug report(s):\n"]
        
        for i, report in enumerate(self.completed_reports, 1):
            summary_lines.append(f"**Bug Report #{i}:**")
            summary_lines.append(f"  • Bug ID: {report.bug_id}")
            summary_lines.append(f"  • Status: {report.status}")
            summary_lines.append(f"  • Solved: {'Yes' if report.solved else 'No'}")
            summary_lines.append(f"  • Work Done: {report.progress_note.split(' - ', 1)[1] if ' - ' in report.progress_note else report.progress_note}")
            summary_lines.append("")
        
        summary_lines.append("This information has been saved to the bug tracking system.")
        return "\n".join(summary_lines)
    
    def add_user_message(self, content: str) -> None:
        """Add user message to conversation history."""
        self.messages.append({"role": "user", "content": content})
    
    def run_interactive(self) -> None:
        """Run the interactive bug reporting conversation."""
        print("\n" + "="*60)
        print("BUG REPORTING CHATBOT")
        print("="*60 + "\n")
        
        # Let LLM start the conversation
        initial_response = self.get_bot_response()
        print(f"Bot: {initial_response}\n")
        self.trace.append({"type": "message", "role": "assistant", "content": initial_response})
        
        while self.turn_count < self.max_turns:
            # Get user input
            try:
                user_input = input("You: ").strip()
            except EOFError:
                # End of input stream - graceful exit
                break
            
            if user_input.lower() == "quit":
                break
            
            if not user_input:
                continue
            
            self.turn_count += 1
            self.add_user_message(user_input)
            self.trace.append({"type": "message", "role": "user", "content": user_input})
            
            # Check if user wants to end conversation BEFORE getting bot response
            # This way we detect end signals before bot generates farewell
            end_detected = self._should_end_conversation()
            print(f"[DEBUG_LOOP] End detection returned: {end_detected}")
            if end_detected:
                # FINAL ANALYSIS: Analyze entire conversation and extract all reports
                self.completed_reports = self._analyze_conversation_for_reports()
                
                # Show final summary to user
                summary = self._get_final_summary()
                print(f"Bot: {summary}\n")
                self.trace.append({"type": "message", "role": "assistant", "content": summary})
                break
            
            # Get next bot response (only if conversation hasn't ended)
            bot_response = self.get_bot_response()
            print(f"Bot: {bot_response}\n")
            self.trace.append({"type": "message", "role": "assistant", "content": bot_response})
    
    def get_structured_output(self) -> ConversationOutput:
        """Generate final structured output."""
        if self.completed_reports:
            return ConversationOutput(
                success=True,
                reports=self.completed_reports
            )
        
        return ConversationOutput(
            success=False,
            reports=[]
        )
